import time
import random
import requests
import csv
import torch
from transformers import pipeline
from transformers import AutoTokenizer
from typing import List, Dict, Any
from ConstitutionalAiTuning.prompting.prompt_template import PromptTemplate
from tqdm import tqdm
import asyncio
from concurrent.futures import ThreadPoolExecutor

class ModelInteractor:
    def __init__(self, hf_model: str = None, hf_api_key: str = None, endpoint_url: str = None, use_mock: bool = False,  system_message: str = ""):
        """
        Initialize the ModelInteractor with either a text generation pipeline, OpenAI API key and model, or API     credentials, and an option to use mock responses.
    
        Args:
            hf_model (str): The name of the Hugging Face model.
            hf_api_key (str, optional): The token for the API authentication on the Hugging Face Hub. Defaults to None.
            endpoint_url (str, optional): The endpoint URL of the API for models hosted on a dedicated server. Will be set  to URL of the Hugging Face Inference API if no URL is provided.
            use_mock (bool): If True, uses a mock function instead of the actual LLM for responses.
            system_message (str, optional): The system message to initialize the chat history. Defaults to an empty string.
        """
        if hf_model is None :
            raise ValueError("hf_model must be provided")
        
        if hf_api_key is not None:
            if endpoint_url is not None:
                self.api_url = endpoint_url
            else:
                self.api_url = f"https://api-inference.huggingface.co/models/{hf_model}"
            self.tokenizer = AutoTokenizer.from_pretrained(hf_model)
            self.text_gen_pipeline = None
        else:
            self.api_url = None
            self.tokenizer = None
            self.text_gen_pipeline = pipeline(
                "text-generation",
                model=self.hf_model,
                torch_dtype=torch.bfloat16,
                device_map="auto",
            )

        self.system_message = system_message
        self.chat_history = ""

        self.hf_api_key = hf_api_key
        self.use_mock = use_mock

    def mock_execute_llm_request(self, prompt: str, **kwargs) -> str:
        """
        Mock method to simulate LLM responses.

        Args:
            prompt (str): The prompt to be sent to the mock LLM.

        Returns:
            str: A mock output.
        """
        prompt_str = str(prompt)
        if len(prompt_str) > 210:
            return f"Mock response for prompt: {prompt_str[:100]}...{prompt_str[-100:]}"
        else:
            return f"Mock response for prompt: {prompt_str}"

    def execute_llm_request(
        self,
        prompt: str,
        max_prompt_tokens: int = 2048,
        max_new_tokens: int = 256,
        do_sample: bool = True,
        temperature: float = 0.7,
        top_k: int = 50,
        top_p: float = 0.95,
        use_cache: bool = True,
        add_generation_prompt: bool = True,
        verbose: bool = False

    ) -> str:
        """
        Executes a request to the LLM using the provided pipeline, OpenAI API, API, or a mock function based on the use_mock flag.

        Args:
            prompt (str): The prompt to be sent to the LLM.
            max_prompt_tokens (int): The maximum number of tokens allowed in the prompt. Default is 2048.
            max_new_tokens (int): The maximum number of new tokens to generate. Default is 256.
            do_sample (bool): Whether to sample the output. Default is True.
            temperature (float): Sampling temperature. Default is 0.7.
            top_k (int): The number of highest probability vocabulary tokens to keep for top-k-filtering. Default is 50.
            top_p (float): Nucleus filtering (top-p) cumulative probability. Default is 0.95.
            use_cache (bool): Whether to use the model's inference cache. Default is True.
            add_generation_prompt (bool): If True, adds the generation prompt to the chat prompt. Default is True.
            verbose (bool): If True, prints a short form of the prompt and the number of tokens it contains. Defaults to False.

        Returns:
            str: The output generated by the local LLM, API, or the mock function.
        """
        if self.use_mock:
            return self.mock_execute_llm_request(prompt)

        chat_prompt = self.tokenizer.apply_chat_template(
            prompt, tokenize=False, add_generation_prompt=add_generation_prompt
        )

        # Remove the end of sequence token if add_generation_prompt is False
        if not add_generation_prompt:
            chat_prompt = chat_prompt.rsplit('</s>', 1)[0]

        # Tokenize the prompt and check if it's too long
        tokenized_prompt = self.tokenizer.encode(chat_prompt)
        if verbose:
            print("##### Prompt request:\n", chat_prompt[:200] + "..." + chat_prompt[-200:] if len(chat_prompt) >= 401 else chat_prompt)
            print("##### Prompt length:", len(tokenized_prompt))
        if len(tokenized_prompt) > max_prompt_tokens:
            raise ValueError(
                f"The prompt is too long. Maximum allowed tokens: {max_prompt_tokens}, current tokens: {len (tokenized_prompt)}"
            )

        if self.hf_api_key is None:
            response = self.pipeline(
                chat_prompt,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                use_cache=use_cache,
            )
            generated_text = response.json()[0]["generated_text"]
            return generated_text[len(chat_prompt):]

        payload = {
            "inputs": chat_prompt,
            "parameters": {
                "max_new_tokens": max_new_tokens,
                "do_sample": do_sample,
                "temperature": temperature,
                "top_k": top_k,
                "top_p": top_p,
                "use_cache": use_cache,
            }
        }
        headers = {
            "Accept": "application/json",
            "Authorization": f"Bearer {self.hf_api_key}",
            "Content-Type": "application/json"
        }
        response = requests.post(self.api_url, headers=headers, json=payload)
        response.raise_for_status()
        generated_text = response.json()[0]["generated_text"]

        # Remove the chat prompt from the generated text if included
        if generated_text.startswith(chat_prompt):
            return generated_text[len(chat_prompt):]
        else:
            return generated_text
                
    def run_single_answer_improvement(
        self, 
        prompts: List[Dict[str, Any]], 
        principles: Dict[str, Any],
        prompt_index: int = None,
        verbose: bool = False
    ) -> Dict[str, Any]:
        """
        Runs a single answer improvement cycle (initial answer, critique, revision) 
        for a specified or randomly selected input prompt, with control over verbosity.
        Returns a dictionary with the interaction history.

        Args:
            prompts (list): A list of prompts.
            principles (dict): Principles for improving the initial answer generated by the model.
            prompt_index (int, optional): Index of the specific prompt to be used. If None, a random prompt is selected.
            verbose (bool): If True, prints the results and execution time. Defaults to False.

        Returns:
            dict: Dictionary containing the selected prompt and interaction history (initial answer, critique, revision).
        """
        # Select a specific or random prompt
        if prompt_index is not None and prompt_index < len(prompts):
            prompt = prompts[prompt_index]
        else:
            prompt = random.choice(prompts)

        cai_prompts = PromptTemplate(
            user_prompt=prompt["prompt"],
            principles=principles
        )

        # Define the prompts
        prompt_methods = {
            "initial_answer": cai_prompts.generate_initial_answer_prompt,
            "critique": cai_prompts.generate_critique_prompt,
            "revision": cai_prompts.generate_revision_prompt,
        }

        # Generate the responses for each prompt
        for prompt_name, (prompt_method) in prompt_methods.items():
            if verbose:
                print("#############################################")
                print(f"##### {prompt_name.replace('_', ' ').title()}\n")

            start_time = time.time()
            response = self.execute_llm_request(prompt_method(), verbose=verbose)
            # Remove "Revision: " or "Critique: " from the beginning of the response
            response = response.replace("Revision: ", "").replace("Critique: ", "")    
            setattr(cai_prompts, prompt_name, response)
            end_time = time.time()

            if verbose:
                print(f"##### Response:\n {response}")
                print("##### Time Taken:", end_time - start_time, "seconds\n")

        return cai_prompts.get_history()

    def run_single_comparison(
        self, 
        prompts: List[Dict[str, Any]], 
        principles: Dict[str, Any],
        prompt_index: int = None,
        verbose: bool = False
    ) -> Dict[str, Any]:
        """
        Runs a single comparison cycle (comparison answers, chain of thought, selected answer)
        for a specified or randomly selected input prompt, with control over verbosity.
        Returns a dictionary with the interaction history.

        Args:
            prompts (list): A list of prompts.
            principles (dict): Principles for comparing the initial answers generated by the model.
            prompt_index (int, optional): Index of the specific prompt to be used. If None, a random prompt is selected.
            verbose (bool): If True, prints the results and execution time. Defaults to False.

        Returns:
            dict: Dictionary containing the selected prompt and comparison history (comparison answers, chain of thought, selected answer).
        """
        # Select a specific or random prompt
        if prompt_index is not None and prompt_index < len(prompts):
            prompt = prompts[prompt_index]
        else:
            prompt = random.choice(prompts)

        cai_prompts = PromptTemplate(
            user_prompt=prompt["prompt"],
            principles=principles
        )

        # Define the prompts
        prompt_methods = {
            "comparison_answer_1": (cai_prompts.generate_initial_answer_prompt, True),
            "comparison_answer_2": (cai_prompts.generate_initial_answer_prompt, True),
            "chain_of_thought": (cai_prompts.generate_chain_of_thought_comparison_prompt, False),
            "selected_answer": (cai_prompts.generate_selected_answer_comparison_prompt, False),
        }

        # Generate the responses for each prompt
        for prompt_name, (prompt_method, add_generation_prompt) in prompt_methods.items():
            if verbose:
                print("#############################################")
                print(f"##### {prompt_name.replace('_', ' ').title()}\n")

            start_time = time.time()
            # Do not use the inference cache for generating the comparison answer 2 (otherwise always it will be the same as comparison answer 1)
            use_cache = False if prompt_name == "comparison_answer_2" else True
            response = self.execute_llm_request(prompt_method(include_system_prompt=False), use_cache=use_cache, add_generation_prompt=add_generation_prompt, verbose=verbose)
            setattr(cai_prompts, prompt_name, response)
            end_time = time.time()

            if verbose:
                print(f"##### Response:\n {response}")
                print(" ##### Time Taken:", end_time - start_time, "seconds\n")

        return cai_prompts.get_history()
    
    def run_answer_improvement_loop(
        self, 
        prompts: List[Dict[str, Any]], 
        principles: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """b
        Loops over prompts for generating answers, critiques, and revisions.
        Uses the run_single_interaction method for each prompt.

        Args:
            prompts (list): A list of prompts.
            principles (dict): Principles for improving the initial answers generated by the model.

        Returns:
            list: A list of dictionaries, each containing the prompt and interaction history (initial answer,     critique, revision).
        """
        sft_data = []

        for index, prompt in tqdm(enumerate(prompts), total=len(prompts)):
            # Use run_single_interaction for each prompt
            interaction_result = self.run_single_answer_improvement(
                prompts, 
                principles, 
                prompt_index=index, 
                verbose=False  # Set verbose to False to avoid printing during loop
            )
            sft_data.append(interaction_result)

        return sft_data

    def run_comparison_loop(
        self,
        prompts: List[Dict[str, Any]],
        principles: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Loops over prompts for generating comparison answers.
        Uses the run_single_comparison method for each prompt.

        Args:
            prompts (list): A list of prompts.
            principles (dict): Principles for comparing the initial answers generated by the model.

        Returns:
            list: A list of dictionaries, each containing the prompt and comparison history (comparison answers, chain of thought, selected answer).
        """
        comparison_data = []

        for index, prompt in tqdm(enumerate(prompts), total=len(prompts)):
            # Use run_single_comparison for each prompt
            comparison_result = self.run_single_comparison(
                prompts, 
                principles, 
                prompt_index=index, 
                verbose=False  # Set verbose to False to avoid printing during loop
            )
            comparison_data.append(comparison_result)

        return comparison_data
       
    def run_async_comparison_loop(
        self,
        prompts: List[Dict[str, Any]],
        principles: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Loops over prompts for generating comparison answers.
        Uses the run_single_comparison method for each prompt.

        Args:
            prompts (list): A list of prompts.
            principles (dict): Principles for comparing the initial answers generated by the model.

        Returns:
            list: A list of dictionaries, each containing the prompt and comparison history (comparison answers, chain of   thought, selected answer).
        """
        comparison_data = []

        async def process_prompt(prompt_index):
            # Use run_single_comparison for each prompt
            comparison_result = await asyncio.get_event_loop().run_in_executor(
                None, self.run_single_comparison, prompts, principles, prompt_index
            )
            comparison_data.append(comparison_result)

        async def main():
            loop = asyncio.get_event_loop()
            futures = [
                asyncio.ensure_future(process_prompt(index))
                for index in range(len(prompts))
            ]

            with tqdm(total=len(futures), unit="prompts", desc="Processing prompts") as progress:
                for future in tqdm(asyncio.as_completed(futures), total=len(futures)):
                    await future
                    progress.update(1)

        loop = asyncio.get_event_loop()
        loop.run_until_complete(main())
        loop.close()
        
        return comparison_data
    
    def save_prompts_and_revisions_to_csv(self, responses, output_file):
        """
        Saves the user prompts and revisions in a CSV file with a single column 'text' containing
        the combined user prompt and revision prepared using the chat template.

        Args:
            responses (list): A list of dictionaries containing the interaction history (user_prompt, revision, etc.).
            output_file (str): The path and filename of the output CSV file.
        """
        with open(output_file, mode='w', newline='', encoding='utf-8') as file:
            writer = csv.writer(file)
            # Write the column title "text" in the first line
            writer.writerow(["text"])

            for response in responses:
                user_prompt = response["user_prompt"]
                revision = response["revision"]

                # Combine user prompt and revision in the correct chat prompt list format
                chat_prompt = [
                    {"role": "user", "content": user_prompt},
                    {"role": "assistant", "content": revision}
                ]

                # Apply the chat template
                combined_prompt = self.tokenizer.apply_chat_template(
                    chat_prompt,
                    tokenize=False
                )

                # Remove newline characters from the combined prompt
                combined_prompt = combined_prompt.replace("\n", "")

                writer.writerow([combined_prompt])

    def save_comparison_data_to_csv(self, responses, output_file):
        """
        Saves the comparison answers and selected answer in a CSV file with the three columns 'prompt', 'chosen', and 'rejected' containing the iinitial prompt, the selected answer, and the rejected answer.

        Args:
            responses (list): A list of dictionaries containing the comparison history (comparison answers, chain of thought, selected answer).
            output_file (str): The path and filename of the output CSV file.
        """
        with open(output_file, mode='w', newline='', encoding='utf-8') as file:
            writer = csv.writer(file, quoting=csv.QUOTE_ALL)
            # Write the column titles "prompt", "chosen", and "rejected" in the first line
            writer.writerow(["prompt", "chosen", "rejected"])

            for response in responses:

                selected_answer = response["selected_answer"].strip().replace('\n', '')
                if selected_answer == '(A)':
                    chosen_answer = response["comparison_answer_1"]
                    rejected_answer = response["comparison_answer_2"]
                elif selected_answer == '(B)':
                    chosen_answer = response["comparison_answer_2"]
                    rejected_answer = response["comparison_answer_1"]
                else:
                    chosen_answer = '--unclear--'
                    rejected_answer = '--unclear--'

                writer.writerow([response["user_prompt"], chosen_answer, rejected_answer])
                
    def chat_with_model(self, user_prompt: str, reset: bool = False, **kwargs) -> str:
        """
        Interacts with the model as in a chat, sending the current chat history and user prompt,
        and updating the chat history with the assistant's response.
    
        Args:
            user_prompt (str): The user's prompt.
            reset (bool): If True, resets the chat history. Defaults to False.
            **kwargs: Additional keyword arguments to pass to the execute_llm_request method.
    
        Returns:
            str: The assistant's response.
        """

        # Reset the chat history if requested
        if reset:
            self.chat_history = ""

        # Format the user prompt
        user_prompt_dict = {"role": "user", "content": user_prompt}
    
        # Combine the system message, chat history, and user prompt in the correct format
        chat_prompt = []
        if self.system_message:
            chat_prompt.append({"role": "system", "content": self.system_message})
        if self.chat_history:
            chat_prompt.extend(eval(self.chat_history))
        chat_prompt.append(user_prompt_dict)

        # log chat prompt
        print(chat_prompt)

        # Send the chat prompt to the LLM
        response = self.execute_llm_request(chat_prompt, **kwargs)
    
        # Update the chat history with the user prompt and assistant response
        self.chat_history = str([user_prompt_dict, {"role": "assistant", "content": response}])
    
        return response