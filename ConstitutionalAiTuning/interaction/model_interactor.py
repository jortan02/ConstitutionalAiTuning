import time
import random
import transformers
from typing import List, Dict, Any
from ConstitutionalAiTuning.prompting.prompt_template import PromptTemplate

class ModelInteractor:
    def __init__(self, pipeline: transformers.pipelines.text_generation.TextGenerationPipeline):
        """
        Initialize the ModelInteractor with a text generation pipeline.

        Args:
            pipeline (transformers.pipelines.text_generation.TextGenerationPipeline): 
                A pipeline for text generation.
        """
        if not isinstance(pipeline, transformers.pipelines.text_generation.TextGenerationPipeline):
            raise TypeError("pipeline must be an instance of transformers.pipelines.text_generation.TextGenerationPipeline")
        self.pipeline = pipeline

    def execute_llm_request(
        self, 
        prompt: str, 
        max_new_tokens: int = 256,
        do_sample: bool = True,
        temperature: float = 0.7,
        top_k: int = 50,
        top_p: float = 0.95
    ) -> str:
        """
        Executes a request to the LLM using the provided pipeline.

        Args:
            prompt (str): The prompt to be sent to the LLM.
            max_new_tokens (int): The maximum number of new tokens to generate. Default is 256.
            do_sample (bool): Whether to sample the output. Default is True.
            temperature (float): Sampling temperature. Default is 0.7.
            top_k (int): The number of highest probability vocabulary tokens to keep for top-k-filtering. Default is 50.
            top_p (float): Nucleus filtering (top-p) cumulative probability. Default is 0.95.

        Returns:
            str: The output generated by the LLM.
        """
        chat_prompt = self.pipeline.tokenizer.apply_chat_template(
            prompt, tokenize=False, add_generation_prompt=True
        )

        outputs = self.pipeline(
            chat_prompt,
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            return_full_text=False
        )
        return outputs[0]["generated_text"]

    def run_single_interaction(
        self, 
        questions: List[Dict[str, Any]], 
        constitution_settings: Dict[str, Any],
        question_index: int = None,
        verbose: bool = True
    ) -> Dict[str, Any]:
        """
        Runs a single interaction cycle (initial answer, critique, revision) 
        for a specified or randomly selected input prompt, with control over verbosity.
        Returns a dictionary with the interaction history.

        Args:
            questions (list): A list of questions.
            constitution_instructions (dict): Instructions for generating the prompts loaded from the constitution.
            question_index (int, optional): Index of the specific question to be used. If None, a random question is selected.
            verbose (bool): If True, prints the results and execution time. Defaults to True.

        Returns:
            dict: Dictionary containing the selected question and interaction history (initial answer, critique, revision).
        """
        # Select a specific or random prompt
        if question_index is not None and question_index < len(questions):
            prompt = questions[question_index]
        else:
            prompt = random.choice(questions)

        cai_prompts = PromptTemplate(
            input=prompt["input_prompt"],
            prompt_instructions=constitution_settings
        )

        interaction_history = {"input_prompt": prompt["input_prompt"]}

        # Process each stage (initial answer, critique, revision) and print if verbose is True
        for stage in ['initial_answer', 'critique', 'revision']:
            start_time = time.time()
            generated_prompt = getattr(cai_prompts, f'generate_{stage}_prompt')()
            response = self.execute_llm_request(generated_prompt)
            end_time = time.time()

            setattr(cai_prompts, stage, response)
            interaction_history[stage] = response

            if verbose:
                print(f"{stage.replace('_', ' ').title()}: {response}")
                print("Time Taken:", end_time - start_time, "seconds\n")

        return interaction_history

    def run_interaction_loop(
        self, 
        questions: List[Dict[str, Any]], 
        constitution_settings: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Loops over prompts for generating answers, critiques, and revisions.
        Uses the run_single_interaction method for each prompt.

        Args:
            questions (list): A list of questions.
            constitution_instructions (dict): Instructions for generating the prompts loaded from the constitution.

        Returns:
            list: A list of dictionaries, each containing the question and interaction history (initial answer,     critique, revision).
        """
        sft_data = []

        for index, question in enumerate(questions):
            # Use run_single_interaction for each prompt
            interaction_result = self.run_single_interaction(
                questions, 
                constitution_settings, 
                question_index=index, 
                verbose=False  # Set verbose to False to avoid printing during loop
            )
            sft_data.append(interaction_result)

        return sft_data
